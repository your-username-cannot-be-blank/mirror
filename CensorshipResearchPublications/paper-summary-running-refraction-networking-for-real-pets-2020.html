
    <html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
        <style>
    th, td {
      border: 1px solid black;
      border-collapse: collapse;
    }
    th, td {
      word-wrap: break-word;
      padding: 5px;
    }
    table {
      width: 100%;
      max-width: 100%;
      border: 1px solid black;
      border-collapse: collapse;
      table-layout: fixed;
      word-wrap: break-word
    }
    code {
      word-wrap: break-word;
    }
    pre {
        white-space: pre-wrap;
    }
    </style>
        <title>paper-summary-running-refraction-networking-for-real-pets-2020</title>
      </head>
      <body>
        <table border="1" width="100%" cellpadding="5">
          <tr>
            <th style="width: 10%;">Ник</th>
            <th>Пост</th>
            <th style="width: 10%;">Дата</th>
          </tr>
    <tr><td>tango</td><td><p>Running Refraction Networking for Real<br>
Benjamin VanderSloot, Sergey Frolov, Jack Wampler, Sze Chuen Tan, Irv Simpson, Michalis Kallitsis, J. Alex Halderman, Nikita Borisov, Eric Wustrow<br>
<a href="https://censorbib.nymity.ch/#VanderSloot2020a" rel="noopener nofollow ugc">https://censorbib.nymity.ch/#VanderSloot2020a</a><br>
<a href="https://www.youtube.com/watch?v=ZDYKdy7lCRY&amp;list=PLWSQygNuIsPeBNObt8cV5KOsyomz3-B8_&amp;index=73" rel="noopener nofollow ugc">Presentation video</a></p>
<p>The paper reflects on one year of running a <a href="https://refraction.network/" rel="noopener nofollow ugc">refraction networking</a> system with real users at a real ISP. Following a one-month pilot deployment in 2017 (<a href="https://censorbib.nymity.ch/#Frolov2017a" rel="noopener nofollow ugc">“An ISP-Scale Deployment of TapDance”</a>), the team began operation in earnest in October 2018. The deployment used <a href="https://censorbib.nymity.ch/#Wustrow2014a" rel="noopener nofollow ugc">TapDance</a> as the refraction networking system, with the client base being a select subset of <a href="https://psiphon.ca/" rel="noopener nofollow ugc">Psiphon</a> users. The cooperating ISP was <a href="https://www.merit.edu/" rel="noopener nofollow ugc">Merit Network</a>.</p>
<p>The original TapDance paper considered a simplified ISP model, where a single refraction networking station could cover all of an ISP’s traffic. In reality, an ISP has multiple uplinks, and therefore a single station does not suffice; multiple stations must work together as a distributed system. To make this work, the authors divided the stations’ responsibilities between multiple independent <em>detectors</em> and a single centralized <em>proxy</em>. The detectors search TLS traffic for TapDance tags, and forward only the matching flows to the proxy. (This division of responsibilities is similar to the split between controllers/switches and the secret proxy in <a href="https://ntc.party/t/paper-summary-siegebreaker-an-sdn-based-practical-decoy-routing-system-pets-2020/867/2">SiegeBreaker</a>.) A client prefixes each of its decoy flows with a randomly generated <em>session identifier</em>. The centralized proxy stitches together flows that share the same session identifier to form one long-term session. This kind of identifier-based multiplexing is necessary because (for technical reasons) a single TapDance flow can only be used for a limited time before it must be discarded, and because packets in a flow may pass by different detectors, which do not share state with each other.</p>
<p>The deployment supported about 33,000 unique users per month, with a peak goodput of 500 Mbps. There were four detectors installed at the ISP, which collectively processed 5,000–20,000 TLS flows per second. The cost of hardware for the detectors and proxy was about $30,000. The estimated cost of rack space and bandwidth, if not donated, as about $35,000 per year. About 559,000 Psiphon clients were TapDance-capable, but because Psiphon is a multi-modal circumvention system that automatically selects the transport with the lowest latency, not all those clients used TapDance all the time. Psiphon’s adaptive transport selection caused an interesting effect during a censorship event: under normal conditions, TapDance-capable clients used TapDance about 10% of the time, but when some of Psiphon’s other transports became temporarily blocked, the fraction rose to 40%. Backend infrastructure that used port scanning to find eligible decoy sites: there were about 3,000 hosts behind the ISP that returned a TLS certificate on port 443, but the number of usable decoys dropped to about 1,500 after filtering for TCP window sizes, TLS ciphers, and other TapDance requirements. On average, a client would make one failed decoy connection before finding one that worked.</p>
<p><a href="https://petsymposium.org/2020/files/papers/issue4/popets-2020-0073.pdf#page=13" rel="noopener nofollow ugc">Section 5.3</a> has a nice discussion of ISP concerns related to deploying refraction networking. Each station was permitted <a href="https://en.wikipedia.org/wiki/Rack_unit" rel="noopener nofollow ugc">1U</a> of rack space. The installation could not interfere with any of the ISP’s normal operations—and in particular, the failure of a station could not disrupt any of the ISP’s other duties. (TapDance, of course, was developed for the purpose of working without blocking normal packet flows.) The authors observe that the success of refraction networking “depends on close interactions between the Internet operator and Internet freedom communities.”</p>
<p>Thanks to Eric Wustrow for commenting on a draft of this summary.</p></td><td>2021-06-07T20:33:54.145Z</td></tr>
    </table>
      </body>
    </html>